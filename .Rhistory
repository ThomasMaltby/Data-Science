p <- posterior(bf, iterations=1000)
plot(p)
summary(aov(alldata ~ group, data = q4data))
bf <- summary(anovaBF(alldata ~ group, data = q4data))
bf <- summary(anovaBF(alldata ~ group, data = q4data, rscaleFixed = 1))
summary(aov(Injury ~ Athlete*Stretch, data = q7data))
summary(anovaBF(Injury ~ Athlete*Stretch, data = q7data, rscaleFixed = "ultrawide"))
summary(anovaBF(Injury ~ Athlete*Stretch, data = q7data))
proportionBF(y = 62, N = 100, p = 0.5)
proportionBF(y = 60, N = 100, p = 0.5)
proportionBF(y = 58, N = 100, p = 0.5)
proportionBF(y = 64, N = 100, p = 0.5)
dir()
library(tidyverse, lib.loc = "T:/Applications/Departments/Biology/Rlibs/3.6.0")
library(magrittr, lib.loc = "T:/Applications/Departments/Biology/Rlibs/3.6.0")
nums <- sample(1:100, size = 10, replace = FALSE)
#nesting requires reading from the inside out, not very readable
tnums <- log(sqrt(nums))
#intermediate variables
rootnums <- sqrt(nums)
tnums <- log(rootnums)
#using the pipe more readable, doesn't need to be in brackets
#easier to debug
tnums <- nums %>%
sqrt() %>%
log()
biomass2 <- biomass %>%
gather(key = spray,
value = mass,
-rep_tray)
library(tidyverse)
library(here)
library(reshape2)
biomass2 <- biomass %>%
gather(key = spray,
value = mass,
-rep_tray)
biomass2 <- biomass %>%
gather(key = spray,
value = mass,
-rep_tray)
file <- here::here("data", "raw", "biomass.txt")
biomass <- read.table(file, header = TRUE)
file <- here::here("data", "raw", "biomass.txt")
biomass <- read.table(file, header = TRUE)
file <- here::here("data", "raw", "biomass.txt")
biomass <- read.table(file, header = TRUE)
file <- here::here("data", "raw", "biomass.txt")
biomass <- read.table(file, header = TRUE)
biomass <- here::here("data", "raw", "biomass.txt")
biomass2 <- biomass %>%
gather(key = spray,
value = mass,
-rep_tray)
biomass <- here::here("data", "raw", "biomass.txt")
chaff <- read.table(biomass, header = TRUE)
biomass <- here::here("data, "biomass.txt")
biomass <- here::here("data", "biomass.txt")
chaff <- read.table(biomass, header = TRUE)
biomass <- here::here("data", "raw", "biomass.txt")
chaff <- read.table(biomass, header = TRUE)
here()
here::here
here::here()
file <- here::here("data", "raw", "chaff.txt")
chaff <- read.table(file, header = TRUE)
dir()
file <- here::here("data", "raw", "biomass.txt")
chaff <- read.table(biomass, header = TRUE)
here::here()
#use incase packages already installed
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
#SVM takes two parameters, C and Gamma. Gamma defines how far the influence of a single training_seting example reaches, the C parameter trades off correct classification of training_seting examples against maximization of the decision function’s margin.
#define parameters and all combinations of such --> change values
gamma <- seq(0,0.1,0.005)
cost <- 2^(0:5)
parms <- expand.grid(cost=cost, gamma=gamma)
acc_testing_set <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL
for(i in 1:NROW(parms)){
learn_svm <- svm(diagnosis~., data=training_set, gamma=parms$gamma[i], cost=parms$cost[i])
pre_svm <- predict(learn_svm, testing_set[,-1])
accuracy1 <- confusionMatrix(pre_svm, testing_set$diagnosis)
accuracy2[i] <- accuracy1$overall[1]
}
library(here)
library(neldermead)
library(optimsimplex)
library(optimbase)
library(lavaan) # structural equation modelling
library(sem) # structural equation modelling
library(factoextra) # running a PCA
library(magrittr)
library(PerformanceAnalytics) # better presentation of data - maybe remove
library(rlang) # for ggplot2
library(ggplot2) # presentation of data
library(GGally) # presentation of data
library(recipes)
library(caret) # machine learning
library(e1071) # running naiveBayes
library(randomForest) # running randomForest
library(class) # running knn
library(C50) # algorithm for creating a decision tree
library(rpart) # rpart decision tree
library(highcharter) #used to display various ML outputs
library(corrplot) # display PCA results
library(psych) # run EFA and parallel analysis
library(GPArotation) # used to rotate a matrix for factor analysis
library(plotly)
library(corrplot)
#using the here package to make navigating relative folder paths easier
file <- here::here("data", "raw", "data.csv")
wbcd <- read.csv(file, header = TRUE, stringsAsFactors = FALSE)
#an extra column with no data exists in the file, removed to keep tidy
wbcd$X <- NULL
#remove the first column containing IDs, not useful for the analysis
wbcd <- wbcd[,-1]
#replace M/B character type with Malignant/Benign factors to be used in defining classes in analyses
wbcd$diagnosis[wbcd$diagnosis == "B"] <- "Benign"
wbcd$diagnosis[wbcd$diagnosis == "M"] <- "Malignant"
wbcd$diagnosis <- as.factor(wbcd$diagnosis)
nrows <- NROW(wbcd)
set.seed(666)				            ## fix random value for reproducability
# find a paper that justifies the 70/30 split
index <- sample(1:nrows, 0.7 * nrows)	## shuffle and divide
#training_set <- wbcd		        	      ## 569 testing_set data (70%)
training_set <- wbcd[index,]			        ## 398 testing_set data (30%)
testing_set <- wbcd[-index,]  		        ## 171 testing_set data (30%)
#SVM takes two parameters, C and Gamma. Gamma defines how far the influence of a single training_seting example reaches, the C parameter trades off correct classification of training_seting examples against maximization of the decision function’s margin.
#define parameters and all combinations of such --> change values
gamma <- seq(0,0.1,0.005)
cost <- 2^(0:5)
parms <- expand.grid(cost=cost, gamma=gamma)
acc_testing_set <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL
for(i in 1:NROW(parms)){
learn_svm <- svm(diagnosis~., data=training_set, gamma=parms$gamma[i], cost=parms$cost[i])
pre_svm <- predict(learn_svm, testing_set[,-1])
accuracy1 <- confusionMatrix(pre_svm, testing_set$diagnosis)
accuracy2[i] <- accuracy1$overall[1]
}
acc <- data.frame(p= seq(1,NROW(parms)), cnt = accuracy2)
opt_p <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of parameter is", opt_p$p, "(accuracy :", opt_p$cnt,") in SVM")
#plot of accuracy against number of parametrs, change how this is performed
hchart(acc, 'line', hcaes(p, cnt)) %>%
hc_title(text = "Accuracy With Varying Parameters (SVM)") %>%
hc_subtitle(text = sub) %>%
hc_add_theme(hc_theme_google()) %>%
hc_xAxis(title = list(text = "Number of Parameters")) %>%
hc_yAxis(title = list(text = "Accuracy"))
#rerun the SVM with optimal parameters to show best prediction performance
learn_imp_svm <- svm(diagnosis~., data=training_set, cost=parms$cost[opt_p$p], gamma=parms$gamma[opt_p$p])
pre_imp_svm <- predict(learn_imp_svm, testing_set[,-1])
cm_imp_svm <- confusionMatrix(pre_imp_svm, testing_set$diagnosis)
cm_imp_svm
#k means takes different values of k (number of nearest neighbours) when used in classification
#create a numeric variable to store testing_set statictics
acc_testing_set <- numeric()
#create a loop to training_set knn with changing value of k, testing_set accuracy for each given value of k
for(i in 1:30){
predict <- knn(training_set=training_set[,-1],
testing_set=testing_set[,-1],
cl=training_set[,1],
k=i,
prob=T)
acc_testing_set <- c(acc_testing_set,
mean(predict==testing_set[,1]))
}
#k means takes different values of k (number of nearest neighbours) when used in classification
#create a numeric variable to store testing_set statictics
acc_testing_set <- numeric()
#create a loop to training_set knn with changing value of k, testing_set accuracy for each given value of k
for(i in 1:30){
predict <- knn(train=training_set[,-1],
test=testing_set[,-1],
cl=training_set[,1],
k=i,
prob=T)
acc_testing_set <- c(acc_testing_set,
mean(predict==testing_set[,1]))
}
#create a data frame containing all values of k against accuracies,
acc <- data.frame(k= seq(1,30), cnt = acc_testing_set)
opt_k <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of k is", opt_k$k, "(accuracy :", opt_k$cnt,") in KNN")
#Find alternative method of presentation
hchart(acc, 'line', hcaes(k, cnt)) %>%
hc_title(text = "Accuracy With Varying K (KNN)") %>%
hc_subtitle(text = sub) %>%
hc_add_theme(hc_theme_google()) %>%
hc_xAxis(title = list(text = "Number of Neighbors(k)")) %>%
hc_yAxis(title = list(text = "Accuracy"))
pre_knn <- knn(training_set = training_set[,-1], testing_set = testing_set[,-1], cl = training_set[,1], k=opt_k$k, prob=T)
pre_knn <- knn(train = training_set[,-1], test = testing_set[,-1], cl = training_set[,1], k=opt_k$k, prob=T)
cm_knn  <- confusionMatrix(pre_knn, testing_set$diagnosis)
cm_knn
#training_set the random forst, ntree set to 800 to ensure that each input row predicted a few times as directed by randomForest() documentation
#~ denotes a model (left = dependent, right is independent), diagnosis dependent on any data
#. mean any colums from data that are otherwise not used
learn_rf <- randomForest(diagnosis~.,
data=training_set,
ntree=800,
proximity=T,
importance=T)
#use training_seted model to classify testing_set data
pre_rf   <- predict(learn_rf,
testing_set[,-1])
#Calculates a cross-tabulation of observed and predicted classes with associated statistics.
cm_rf    <- confusionMatrix(pre_rf,
testing_set$diagnosis)
#display the statistics from the confusion matrix
cm_rf
#plot of the random Forest data
plot(learn_rf,
main="Error Rate vs. No. of Trees")
# understand what is going on describe
plot(margin(learn_rf,testing_set$diagnosis))
#important for understanding contributions of data to predictions/improving measures in future
varImpPlot(learn_rf)
#variables to contain statistical data relating to method accuracy
acc_testing_set <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL
for(i in 1:30){
#run the naiveBayes with a series of different laplace values
learn_imp_nb <- naiveBayes(training_set[,-1], training_set$diagnosis, laplace=i)
#testing_set the training_seted model on the testing_set data
p_nb <- predict(learn_imp_nb, testing_set[,-1])
#Calculates a cross-tabulation of observed and predicted classes with associated statistics.
accuracy1 <- confusionMatrix(p_nb, testing_set$diagnosis)
accuracy2[i] <- accuracy1$overall[1]
}
acc <- data.frame(l= seq(1,30), cnt = accuracy2)
opt_l <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of laplace is", opt_l$l, "(accuracy :", opt_l$cnt,") in naiveBayes")
hchart(acc, 'line', hcaes(l, cnt)) %>%
hc_title(text = "Accuracy With Varying Laplace (naiveBayes)") %>%
hc_subtitle(text = sub) %>%
hc_add_theme(hc_theme_google()) %>%
hc_xAxis(title = list(text = "Number of Laplace")) %>%
hc_yAxis(title = list(text = "Accuracy"))
#can I add to the for loop in first chunk starting at 0?
learn_nb <- naiveBayes(training_set[,-1],
training_set$diagnosis)
pre_nb <- predict(learn_nb,
testing_set[,-1])
cm_nb <- confusionMatrix(pre_nb,
testing_set$diagnosis)
cm_nb
acc_testing_set <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL
for(i in 1:50){
learn_imp_c50 <- C5.0(training_set[,-1],training_set$diagnosis,trials = i)
p_c50 <- predict(learn_imp_c50, testing_set[,-1])
accuracy1 <- confusionMatrix(p_c50, testing_set$diagnosis)
accuracy2[i] <- accuracy1$overall[1]
}
acc <- data.frame(t= seq(1,50), cnt = accuracy2)
opt_t <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of trials is", opt_t$t, "(accuracy :", opt_t$cnt,") in C5.0")
hchart(acc, 'line', hcaes(t, cnt)) %>%
hc_title(text = "Accuracy With Varying Trials (C5.0)") %>%
hc_subtitle(text = sub) %>%
hc_add_theme(hc_theme_google()) %>%
hc_xAxis(title = list(text = "Number of Trials")) %>%
hc_yAxis(title = list(text = "Accuracy"))
learn_imp_c50 <- C5.0(training_set[,-1],training_set$diagnosis,trials=opt_t$t)
pre_imp_c50 <- predict(learn_imp_c50, testing_set[,-1])
cm_imp_c50 <- confusionMatrix(pre_imp_c50, testing_set$diagnosis)
cm_imp_c50
learn_rp <- rpart(diagnosis~.,data=training_set,control=rpart.control(minsplit=2))
pre_rp <- predict(learn_rp, testing_set[,-1], type="class")
cm_rp  <- confusionMatrix(pre_rp, testing_set$diagnosis)
cm_rp
learn_pru <- prune(learn_rp, cp=learn_rp$cptable[which.min(learn_rp$cptable[,"xerror"]),"CP"])
pre_pru <- predict(learn_pru, testing_set[,-1], type="class")
cm_pru <-confusionMatrix(pre_pru, testing_set$diagnosis)
cm_pru
#set the colours for the graphs CHANGE
col <- c("#ed3b3b", "#0099ff")
#define layout
par(mfrow=c(2,4))
#
fourfoldplot(cm_imp_svm$table,
color = col,
conf.level = 0,
margin = 1,
main=paste("Tuned SVM (",round(cm_imp_svm$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_knn$table, color = col, conf.level = 0, margin = 1, main=paste("Tune KNN (",round(cm_knn$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_rf$table, color = col, conf.level = 0, margin = 1, main=paste("RandomForest (",round(cm_rf$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_nb$table, color = col, conf.level = 0, margin = 1, main=paste("NaiveBayes (",round(cm_nb$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_imp_c50$table, color = col, conf.level = 0, margin = 1, main=paste("Tune C5.0 (",round(cm_imp_c50$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_rp$table, color = col, conf.level = 0, margin = 1, main=paste("RPart (",round(cm_rp$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_pru$table, color = col, conf.level = 0, margin = 1, main=paste("Prune (",round(cm_pru$overall[1]*100),"%)",sep=""))
#put the data in a new dataframe for editing in the pca
wbcd_pca <- transform(wbcd)
all_pca <- prcomp(wbcd_pca[,-1],
cor=TRUE,
scale = TRUE)
summary(all_pca)
mean_pca <- prcomp(wbcd_pca[,c(2:11)],
scale = TRUE)
summary(mean_pca)
se_pca <- prcomp(wbcd_pca[,c(12:21)],
scale = TRUE)
summary(se_pca)
worst_pca <- prcomp(wbcd_pca[,c(22:31)],
scale = TRUE)
summary(worst_pca)
fviz_eig(all_pca,
addlabels=TRUE,
ylim=c(0,60),
geom = c("bar", "line"),
barfill = "pink",
barcolor="grey",
linecolor = "red",
ncp=10)+
labs(title = "Cancer All Variances - PCA",
x = "Principal Components", y = "% of variances")
fviz_eig(mean_pca, addlabels=TRUE,
ylim=c(0,60),
geom = c("bar", "line"),
barfill = "pink",
barcolor="grey",
linecolor = "red",
ncp=10)+
labs(title = "Cancer Mean Variances - PCA",
x = "Principal Components", y = "% of variances")
all_var <- get_pca_var(all_pca)
all_var
summary(all_var$contrib)
corrplot(mean_var$contrib, is.corr=FALSE)
mean_var <- get_pca_var(mean_pca)
mean_var
corrplot(mean_var$contrib, is.corr=FALSE)
#Correlation between variables and PCA
library("corrplot")
corrplot(all_var$cos2, is.corr=FALSE)
#put the data in a new dataframe for editing in the pca
wbcd_sem <- transform(wbcd)
mean_sem <- fa.parallel(wbcd_sem[,c(2:11)],
fm = "minres",
fa = "fa")
summary(mean_sem)
threefactor <- fa(wbcd_sem[,c(2:11)],
nfactors = 3,
rotate = "oblimin",
fm="ml")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
threefactor <- fa(wbcd_sem[,c(2:11)],
nfactors = 2,
rotate = "oblimin",
fm="ml")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
threefactor <- fa(wbcd_sem[,c(2:11)],
nfactors = 3,
rotate = "oblimin",
fm="ml")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
threefactor <- fa(wbcd_sem[,c(2:11)],
nfactors = 2,
rotate = "oblimin",
fm="ml")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
#see how well this test performs, maybe compare model to the factors predicted by the PCA
#model specificationm, based upon the exploratory factor analysis
# cancer.model <- '
ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
ML3 =~ smoothness_mean
ML1 ~~ ML3
ML2 ~~ ML3'
cancer.model <- '
#see how well this test performs, maybe compare model to the factors predicted by the PCA
#model specificationm, based upon the exploratory factor analysis
# cancer.model <- '
# ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
#
# ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
#
# ML3 =~ smoothness_mean
#
# ML1 ~~ ML3
#
# ML2 ~~ ML3'
cancer.model <- '
ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + texture_mean
ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean + concavity_mean + smoothness_mean
ML1 ~~ ML2
'
#specify the data to be used, can observe the correlation/covariance matrices
dataset <- wbcd_sem[,c(2:11)]
cor_mat <- cor(dataset)
cov_mat <- cov(dataset)
#fit the model to the data using confirmatory factor analysis
fit <- cfa(model = cancer.model, data = dataset, estimator="GLS")
# ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
#
# ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
#
# ML3 =~ smoothness_mean
#
# ML1 ~~ ML3
#
# ML2 ~~ ML3'
#specify the data to be used, can observe the correlation/covariance matrices
dataset <- wbcd_sem[,c(2:11)]
cor_mat <- cor(dataset)
cov_mat <- cov(dataset)
#fit the model to the data using confirmatory factor analysis
fit <- cfa(model = cancer.model, data = dataset, estimator="GLS")
threefactor <- fa(wbcd_sem[,c(2:11)],
nfactors = 3,
rotate = "oblimin",
fm="ml")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
#see how well this test performs, maybe compare model to the factors predicted by the PCA
#model specification, based upon the exploratory factor analysis
cancer.model <- '
# ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
#
# ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
#
# ML3 =~ smoothness_mean
#
# ML1 ~~ ML3
#
# ML2 ~~ ML3'
#specify the data to be used, can observe the correlation/covariance matrices
dataset <- wbcd_sem[,c(2:11)]
cor_mat <- cor(dataset)
cov_mat <- cov(dataset)
#fit the model to the data using confirmatory factor analysis
fit <- cfa(model = cancer.model, data = dataset, estimator="GLS")
#use incase packages already installed
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
#see how well this test performs, maybe compare model to the factors predicted by the PCA
#model specificationm, based upon the exploratory factor analysis
cancer.model <- '
ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
ML3 =~ smoothness_mean
ML1 ~~ ML3
ML2 ~~ ML3'
#specify the data to be used, can observe the correlation/covariance matrices
dataset <- wbcd_sem[,c(2:11)]
cor_mat <- cor(dataset)
cov_mat <- cov(dataset)
#fit the model to the data using confirmatory factor analysis
fit <- cfa(model = cancer.model, data = dataset, estimator="GLS")
library(here)
library(neldermead)
library(optimsimplex)
library(optimbase)
library(lavaan) # structural equation modelling
library(sem) # structural equation modelling
library(factoextra) # running a PCA
library(magrittr)
library(PerformanceAnalytics) # better presentation of data - maybe remove
library(rlang) # for ggplot2
library(ggplot2) # presentation of data
library(GGally) # presentation of data
library(recipes)
library(caret) # machine learning
library(e1071) # running naiveBayes
library(randomForest) # running randomForest
library(class) # running knn
library(C50) # algorithm for creating a decision tree
library(rpart) # rpart decision tree
library(highcharter) #used to display various ML outputs
library(corrplot) # display PCA results
library(psych) # run EFA and parallel analysis
library(GPArotation) # used to rotate a matrix for factor analysis
library(plotly)
library(corrplot)
#using the here package to make navigating relative folder paths easier
file <- here::here("data", "raw", "data.csv")
wbcd <- read.csv(file, header = TRUE, stringsAsFactors = FALSE)
#an extra column with no data exists in the file, removed to keep tidy
wbcd$X <- NULL
#remove the first column containing IDs, not useful for the analysis
wbcd <- wbcd[,-1]
#replace M/B character type with Malignant/Benign factors to be used in defining classes in analyses
wbcd$diagnosis[wbcd$diagnosis == "B"] <- "Benign"
wbcd$diagnosis[wbcd$diagnosis == "M"] <- "Malignant"
wbcd$diagnosis <- as.factor(wbcd$diagnosis)
#see how well this test performs, maybe compare model to the factors predicted by the PCA
#model specificationm, based upon the exploratory factor analysis
cancer.model <- '
ML1 =~ radius_mean + perimeter_mean + area_mean + concave.points_mean + concavity_mean + texture_mean
ML2 =~ compactness_mean + fractal_dimension_mean + symmetry_mean
ML3 =~ smoothness_mean
ML1 ~~ ML3
ML2 ~~ ML3'
#specify the data to be used, can observe the correlation/covariance matrices
dataset <- wbcd_sem[,c(2:11)]
cor_mat <- cor(dataset)
cov_mat <- cov(dataset)
#fit the model to the data using confirmatory factor analysis
fit <- cfa(model = cancer.model, data = dataset, estimator="GLS")
